{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed29412",
   "metadata": {
    "papermill": {
     "duration": 0.004797,
     "end_time": "2025-05-16T11:48:27.201422",
     "exception": false,
     "start_time": "2025-05-16T11:48:27.196625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Clone code bài tập lớn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c37c20cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T11:48:27.210504Z",
     "iopub.status.busy": "2025-05-16T11:48:27.210183Z",
     "iopub.status.idle": "2025-05-16T11:48:32.543045Z",
     "shell.execute_reply": "2025-05-16T11:48:32.541715Z"
    },
    "papermill": {
     "duration": 5.340236,
     "end_time": "2025-05-16T11:48:32.545726",
     "exception": false,
     "start_time": "2025-05-16T11:48:27.205490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone https://github.com/cuongtv312/marl-delivery.git\n",
    "%cd marl-delivery\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6023979",
   "metadata": {
    "papermill": {
     "duration": 0.003508,
     "end_time": "2025-05-16T11:48:32.553387",
     "exception": false,
     "start_time": "2025-05-16T11:48:32.549879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## QMix\n",
    "Khai báo thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffde4dfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T11:48:32.562604Z",
     "iopub.status.busy": "2025-05-16T11:48:32.561940Z",
     "iopub.status.idle": "2025-05-16T11:48:37.547008Z",
     "shell.execute_reply": "2025-05-16T11:48:37.546237Z"
    },
    "papermill": {
     "duration": 4.991623,
     "end_time": "2025-05-16T11:48:37.548704",
     "exception": false,
     "start_time": "2025-05-16T11:48:32.557081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import random\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5651bc",
   "metadata": {
    "papermill": {
     "duration": 0.003373,
     "end_time": "2025-05-16T11:48:37.556063",
     "exception": false,
     "start_time": "2025-05-16T11:48:37.552690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Chọn device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19542ed0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T11:48:37.564466Z",
     "iopub.status.busy": "2025-05-16T11:48:37.563998Z",
     "iopub.status.idle": "2025-05-16T11:48:37.572330Z",
     "shell.execute_reply": "2025-05-16T11:48:37.571356Z"
    },
    "papermill": {
     "duration": 0.014006,
     "end_time": "2025-05-16T11:48:37.573569",
     "exception": false,
     "start_time": "2025-05-16T11:48:37.559563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "GPU = True\n",
    "device_idx = 0\n",
    "if GPU:\n",
    "    device = torch.device(\"cuda:\" + str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17bee35",
   "metadata": {
    "papermill": {
     "duration": 0.003467,
     "end_time": "2025-05-16T11:48:37.580935",
     "exception": false,
     "start_time": "2025-05-16T11:48:37.577468",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Tạo ReplayBufferGRU để lưu lại những action trước của agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20c3b381",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T11:48:37.589880Z",
     "iopub.status.busy": "2025-05-16T11:48:37.589194Z",
     "iopub.status.idle": "2025-05-16T11:48:37.599615Z",
     "shell.execute_reply": "2025-05-16T11:48:37.598662Z"
    },
    "papermill": {
     "duration": 0.01665,
     "end_time": "2025-05-16T11:48:37.601224",
     "exception": false,
     "start_time": "2025-05-16T11:48:37.584574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReplayBufferGRU:\n",
    "    \"\"\" \n",
    "    Replay buffer for agent with GRU network additionally storing previous action, \n",
    "    initial input hidden state and output hidden state of GRU.\n",
    "    And each sample contains the whole episode instead of a single step.\n",
    "    'hidden_in' and 'hidden_out' are only the initial hidden state for each episode, for GRU initialization.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, hidden_in, hidden_out, state, action, last_action, reward, next_state):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (hidden_in, hidden_out, state, action, last_action, reward, next_state)\n",
    "        self.position = int((self.position + 1) % self.capacity) # ring buffer\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        s_lst, a_lst, la_lst, r_lst, ns_lst, hi_lst, ho_lst = [], [], [], [], [], [], []\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        min_seq_len = float('inf')\n",
    "        for sample in batch:\n",
    "            h_in, h_out, state, action, last_action, reward, next_state = sample\n",
    "            min_seq_len = min(len(state), min_seq_len)\n",
    "            hi_lst.append(h_in)  # h_in: (1, batch_size=1, n_agents, hidden_size)\n",
    "            ho_lst.append(h_out)\n",
    "        hi_lst = torch.cat(hi_lst, dim = -3).detach()  # cat along the batch dim\n",
    "        ho_lst = torch.cat(ho_lst, dim = -3).detach()\n",
    "\n",
    "        # strip sequence length\n",
    "        for sample in batch:\n",
    "            h_in, h_out, state, action, last_action, reward, next_state = sample\n",
    "            sample_len = len(state)\n",
    "            start_idx = int((sample_len - min_seq_len)/2)\n",
    "            end_idx = start_idx + min_seq_len\n",
    "            s_lst.append(state[start_idx:end_idx])\n",
    "            a_lst.append(action[start_idx:end_idx])\n",
    "            la_lst.append(last_action[start_idx:end_idx])\n",
    "            r_lst.append(reward[start_idx:end_idx])\n",
    "            ns_lst.append(next_state[start_idx:end_idx])\n",
    "\n",
    "        return hi_lst, ho_lst, s_lst, a_lst, la_lst, r_lst, ns_lst\n",
    "\n",
    "    def get_length(self):\n",
    "        return len(self.buffer)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d157803c",
   "metadata": {
    "papermill": {
     "duration": 0.003506,
     "end_time": "2025-05-16T11:48:37.608782",
     "exception": false,
     "start_time": "2025-05-16T11:48:37.605276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Xây class RNNAgent cho mỗi agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d8c112f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T11:48:37.618077Z",
     "iopub.status.busy": "2025-05-16T11:48:37.617369Z",
     "iopub.status.idle": "2025-05-16T11:48:37.629478Z",
     "shell.execute_reply": "2025-05-16T11:48:37.628678Z"
    },
    "papermill": {
     "duration": 0.01798,
     "end_time": "2025-05-16T11:48:37.630699",
     "exception": false,
     "start_time": "2025-05-16T11:48:37.612719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNNAgent(nn.Module):\n",
    "    '''\n",
    "    @brief:\n",
    "        evaluate Q value given a state and the action\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_inputs, action_shape, num_actions, hidden_size):\n",
    "        super(RNNAgent, self).__init__()\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.action_shape = action_shape\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs + action_shape*num_actions, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear4 = nn.Linear(hidden_size, action_shape*num_actions)\n",
    "\n",
    "    def forward(self, state, action, hidden_in):\n",
    "        '''\n",
    "        @params:\n",
    "            state: [#batch, #sequence, #agent, #n_feature]\n",
    "            action: [#batch, #sequence, #agent, action_shape]\n",
    "        @return:\n",
    "            qs: [#batch, #sequence, #agent, action_shape, num_actions]\n",
    "        '''\n",
    "        #  to [#sequence, #batch, #agent, #n_feature]\n",
    "        bs, seq_len, n_agents, _= state.shape\n",
    "        state = state.permute(1, 0, 2, 3)\n",
    "        action = action.permute(1, 0, 2, 3)\n",
    "        action = F.one_hot(action, num_classes=self.num_actions)\n",
    "        action = action.view(seq_len, bs, n_agents, -1) # [#sequence, #batch, #agent, action_shape*num_actions]\n",
    "\n",
    "        x = torch.cat([state, action], -1)  # the dim 0 is number of samples\n",
    "        x = x.view(seq_len, bs*n_agents, -1) # change x to [#sequence, #batch*#agent, -1] to meet rnn's input requirement\n",
    "        hidden_in = hidden_in.view(1, bs*n_agents, -1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x, hidden = self.rnn(x, hidden_in)\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = self.linear4(x) # [#sequence, #batch, #agents, #action_shape*#num_actions]\n",
    "        # [#sequence, #batch, #agent, #head * #action]\n",
    "        x = x.view(seq_len, bs, n_agents, self.action_shape, self.num_actions)\n",
    "        hidden = hidden.view(1, bs, n_agents, -1)\n",
    "        # categorical over the discretized actions\n",
    "        qs = F.softmax(x, dim=-1)\n",
    "        qs = qs.permute(1, 0, 2, 3, 4)  # permute back [#batch, #sequence, #agents, #action_shape, #actions]\n",
    "\n",
    "        return qs, hidden\n",
    "    def get_action(self, state, last_action, hidden_in, deterministic=False):\n",
    "        '''\n",
    "        @brief:\n",
    "            for each distributed agent, generate action for one step given input data\n",
    "        @params:\n",
    "            state: [n_agents, n_feature]\n",
    "            last_action: [n_agents, action_shape]\n",
    "        '''\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0).to(device) # add #sequence and #batch: [[#batch, #sequence, n_agents, n_feature]] \n",
    "        last_action = torch.LongTensor(\n",
    "            last_action).unsqueeze(0).unsqueeze(0).to(device)  # add #sequence and #batch: [#batch, #sequence, n_agents, action_shape]\n",
    "        hidden_in = hidden_in.unsqueeze(1) # add #batch: [#batch, n_agents, hidden_dim]\n",
    "        agent_outs, hidden_out = self.forward(state, last_action, hidden_in)  # agents_out: [#batch, #sequence, n_agents, action_shape, action_dim]; hidden_out same as hidden_in\n",
    "        dist = Categorical(agent_outs)\n",
    "\n",
    "        if deterministic:\n",
    "            action = np.argmax(agent_outs.detach().cpu().numpy(), axis=-1)\n",
    "        else:\n",
    "            action = dist.sample().squeeze(0).squeeze(0).detach().cpu().numpy()  # squeeze the added #batch and #sequence dimension\n",
    "        return action, hidden_out  # [n_agents, action_shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aca99df",
   "metadata": {
    "papermill": {
     "duration": 0.003392,
     "end_time": "2025-05-16T11:48:37.638244",
     "exception": false,
     "start_time": "2025-05-16T11:48:37.634852",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Xây class QMix để tính Q_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03bb1233",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T11:48:37.647426Z",
     "iopub.status.busy": "2025-05-16T11:48:37.646648Z",
     "iopub.status.idle": "2025-05-16T11:48:37.657208Z",
     "shell.execute_reply": "2025-05-16T11:48:37.656156Z"
    },
    "papermill": {
     "duration": 0.016453,
     "end_time": "2025-05-16T11:48:37.658644",
     "exception": false,
     "start_time": "2025-05-16T11:48:37.642191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QMix(nn.Module):\n",
    "    def __init__(self, state_dim, n_agents, action_shape, embed_dim=64, hypernet_embed=128, abs=True):\n",
    "        \"\"\"\n",
    "        Critic network class for Qmix. Outputs centralized value function predictions given independent q value.\n",
    "        :param args: (argparse) arguments containing relevant model information.\n",
    "        \"\"\"\n",
    "        super(QMix, self).__init__()\n",
    "\n",
    "        self.n_agents = n_agents\n",
    "        self.state_dim = state_dim * n_agents * action_shape # #features*n_agents\n",
    "        self.action_shape = action_shape\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hypernet_embed = hypernet_embed\n",
    "        self.abs = abs\n",
    "\n",
    "        self.hyper_w_1 = nn.Sequential(nn.Linear(self.state_dim, self.hypernet_embed),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Linear(self.hypernet_embed, self.action_shape * self.embed_dim * self.n_agents))\n",
    "        self.hyper_w_final = nn.Sequential(nn.Linear(self.state_dim, self.hypernet_embed),\n",
    "                                            nn.ReLU(inplace=True),\n",
    "                                           nn.Linear(self.hypernet_embed, self.embed_dim))\n",
    "\n",
    "        # State dependent bias for hidden layer\n",
    "        self.hyper_b_1 = nn.Linear(self.state_dim, self.embed_dim)\n",
    "\n",
    "        # V(s) instead of a bias for the last layers\n",
    "        self.V = nn.Sequential(nn.Linear(self.state_dim, self.embed_dim),\n",
    "                               nn.ReLU(inplace=True),\n",
    "                               nn.Linear(self.embed_dim, 1))\n",
    "\n",
    "    def forward(self, agent_qs, states):\n",
    "        \"\"\"\n",
    "        Compute actions from the given inputs.\n",
    "        @params:\n",
    "            agent_qs: [#batch, #sequence, #agent, #action_shape]\n",
    "            states: [#batch, #sequence, #agent, #features*action_shape]\n",
    "        :param agent_qs: q value inputs into network [batch_size, #agent, action_shape]\n",
    "        :param states: state observation.\n",
    "        :return q_tot: (torch.Tensor) return q-total .\n",
    "        \"\"\"\n",
    "        bs = agent_qs.size(0)\n",
    "        states = states.reshape(-1, self.state_dim)  # [#batch*#sequence, action_shape*#features*#agent]\n",
    "        agent_qs = agent_qs.reshape(-1, 1, self.n_agents*self.action_shape)  # [#batch*#sequence, 1, #agent*#action_shape]\n",
    "        # First layer\n",
    "        w1 = self.hyper_w_1(states).abs() if self.abs else self.hyper_w_1(states)  # [#batch*#sequence, action_shape*self.embed_dim*#agent]\n",
    "        b1 = self.hyper_b_1(states)  # [#batch*#sequence, self.embed_dim]\n",
    "        w1 = w1.view(-1, self.n_agents*self.action_shape, self.embed_dim)  # [#batch*#sequence, #agent*action_shape, self.embed_dim]\n",
    "        b1 = b1.view(-1, 1, self.embed_dim)   # [#batch*#sequence, 1, self.embed_dim]\n",
    "        hidden = F.elu(torch.bmm(agent_qs, w1) + b1)  # [#batch*#sequence, 1, self.embed_dim]\n",
    "\n",
    "        # Second layer\n",
    "        w_final = self.hyper_w_final(states).abs() if self.abs else self.hyper_w_final(states)  # [#batch*#sequence, self.embed_dim]\n",
    "        w_final = w_final.view(-1, self.embed_dim, 1)  # [#batch*#sequence, self.embed_dim, 1]\n",
    "        # State-dependent bias\n",
    "        v = self.V(states).view(-1, 1, 1)  # [#batch*#sequence, 1, 1]\n",
    "        # Compute final output\n",
    "        y = torch.bmm(hidden, w_final) + v  \n",
    "        # Reshape and return\n",
    "        q_tot = y.view(bs, -1, 1) # [#batch, #sequence, 1]\n",
    "        return q_tot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3733b67",
   "metadata": {
    "papermill": {
     "duration": 0.003476,
     "end_time": "2025-05-16T11:48:37.666121",
     "exception": false,
     "start_time": "2025-05-16T11:48:37.662645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Tạo class QMix_Trainer để train QMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e55973b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T11:48:37.674753Z",
     "iopub.status.busy": "2025-05-16T11:48:37.674476Z",
     "iopub.status.idle": "2025-05-16T11:48:37.692342Z",
     "shell.execute_reply": "2025-05-16T11:48:37.691445Z"
    },
    "papermill": {
     "duration": 0.024198,
     "end_time": "2025-05-16T11:48:37.693813",
     "exception": false,
     "start_time": "2025-05-16T11:48:37.669615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QMix_Trainer():\n",
    "    def __init__(self, replay_buffer, n_agents, state_dim, action_shape, action_dim, hidden_dim, hypernet_dim, target_update_interval, lr=0.001, logger=None):\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.action_shape = action_shape\n",
    "        self.n_agents = n_agents\n",
    "        self.target_update_interval = target_update_interval\n",
    "        \n",
    "        self.agent = RNNAgent(state_dim, action_shape, action_dim, hidden_dim).to(device)\n",
    "        self.target_agent = RNNAgent(state_dim, action_shape, action_dim, hidden_dim).to(device)\n",
    "        \n",
    "        self.mixer = QMix(state_dim, n_agents, action_shape,\n",
    "                          hidden_dim, hypernet_dim).to(device)\n",
    "        self.target_mixer = QMix(state_dim, n_agents, action_shape,\n",
    "                          hidden_dim, hypernet_dim).to(device)\n",
    "        \n",
    "        self._update_targets()\n",
    "        self.update_cnt = 0\n",
    "        \n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        self.optimizer = optim.Adam(\n",
    "            list(self.agent.parameters())+list(self.mixer.parameters()), lr=lr)\n",
    "\n",
    "    def sample_action(self):\n",
    "        probs = torch.FloatTensor(\n",
    "            np.ones(self.action_dim)/self.action_dim).to(device)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample((self.n_agents, self.action_shape))\n",
    "\n",
    "        return action.type(torch.FloatTensor).numpy()\n",
    "\n",
    "    def get_action(self, state, last_action, hidden_in, deterministic=False):\n",
    "        '''\n",
    "        @return:\n",
    "            action: w/ shape [#active_as]\n",
    "        '''\n",
    "\n",
    "        action, hidden_out = self.agent.get_action(state, last_action, hidden_in, deterministic=deterministic)\n",
    "\n",
    "        return action, hidden_out\n",
    "\n",
    "    def push_replay_buffer(self, ini_hidden_in, ini_hidden_out, episode_state, episode_action, episode_last_action,\n",
    "                           episode_reward, episode_next_state):\n",
    "        '''\n",
    "        @brief: push arguments into replay buffer\n",
    "        '''\n",
    "        self.replay_buffer.push(ini_hidden_in, ini_hidden_out, episode_state, episode_action, episode_last_action,\n",
    "                                episode_reward, episode_next_state)\n",
    "\n",
    "    def update(self, batch_size):\n",
    "        hidden_in, hidden_out, state, action, last_action, reward, next_state = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "        state = torch.FloatTensor(state).to(device) # [#batch, sequence, #agents, #features*action_shape]\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        action = torch.LongTensor(action).to(device) # [#batch, sequence, #agents, #action_shape]\n",
    "        last_action = torch.LongTensor(last_action).to(device)\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(-1).to(device) # reward is scalar, add 1 dim to be [reward] at the same dim\n",
    "\n",
    "        agent_outs, _ = self.agent(state, last_action, hidden_in) # [#batch, #sequence, #agent, action_shape, num_actions]\n",
    "        \n",
    "        chosen_action_qvals = torch.gather(  # [#batch, #sequence, #agent, action_shape]\n",
    "            agent_outs, dim=-1, index=action.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        qtot = self.mixer(chosen_action_qvals, state) # [#batch, #sequence, 1]\n",
    "\n",
    "        # target q\n",
    "        target_agent_outs, _ = self.target_agent(next_state, action, hidden_out)\n",
    "        target_max_qvals = target_agent_outs.max(dim=-1, keepdim=True)[0] # [#batch, #sequence, #agents, action_shape]\n",
    "        target_qtot = self.target_mixer(target_max_qvals, next_state)\n",
    "        \n",
    "        reward = reward[:, :, 0]  # reward is the same for agents, so take one\n",
    "        targets = self._build_td_lambda_targets(reward, target_qtot)\n",
    "\n",
    "        loss = self.criterion(qtot, targets.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.update_cnt += 1\n",
    "        if self.update_cnt % self.target_update_interval == 0:\n",
    "            self._update_targets()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def _build_td_lambda_targets(self, rewards, target_qs, gamma=0.99, td_lambda=0.6):\n",
    "        '''\n",
    "        @params:\n",
    "            rewards: [#batch, #sequence, 1]\n",
    "            target_qs: [#batch, #sequence, 1]\n",
    "        '''\n",
    "        ret = target_qs.new_zeros(*target_qs.shape)\n",
    "        ret[:, -1] = target_qs[:, -1]\n",
    "        # backwards recursive update of the \"forward view\"\n",
    "        for t in range(ret.shape[1] - 2, -1, -1):\n",
    "            ret[:, t] = td_lambda * gamma * ret[:, t+1] + (rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t+1])\n",
    "        return ret\n",
    "\n",
    "    def _update_targets(self):\n",
    "        for target_param, param in zip(self.target_mixer.parameters(), self.mixer.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        for target_param, param in zip(self.target_agent.parameters(), self.agent.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.agent.state_dict(), path+'_agent')\n",
    "        torch.save(self.mixer.state_dict(), path+'_mixer')\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.agent.load_state_dict(torch.load(path+'_agent'))\n",
    "        self.mixer.load_state_dict(torch.load(path+'_mixer'))\n",
    "\n",
    "        self.agent.eval()\n",
    "        self.mixer.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34925da",
   "metadata": {
    "papermill": {
     "duration": 0.003845,
     "end_time": "2025-05-16T11:48:37.701698",
     "exception": false,
     "start_time": "2025-05-16T11:48:37.697853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8fe7fd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T11:48:37.711004Z",
     "iopub.status.busy": "2025-05-16T11:48:37.710292Z",
     "iopub.status.idle": "2025-05-16T11:48:38.618961Z",
     "shell.execute_reply": "2025-05-16T11:48:38.617896Z"
    },
    "papermill": {
     "duration": 0.914946,
     "end_time": "2025-05-16T11:48:38.620593",
     "exception": false,
     "start_time": "2025-05-16T11:48:37.705647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:596: UserWarning: \u001b[33mWARN: plugin: shimmy.registration:register_gymnasium_envs raised Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py\", line 594, in load_plugin_envs\n",
      "    fn()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/shimmy/registration.py\", line 304, in register_gymnasium_envs\n",
      "    _register_atari_envs()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/shimmy/registration.py\", line 205, in _register_atari_envs\n",
      "    import ale_py\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ale_py/__init__.py\", line 68, in <module>\n",
      "    register_v0_v4_envs()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ale_py/registration.py\", line 179, in register_v0_v4_envs\n",
      "    _register_rom_configs(legacy_games, obs_types, versions)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ale_py/registration.py\", line 64, in _register_rom_configs\n",
      "    gymnasium.register(\n",
      "    ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: partially initialized module 'gymnasium' has no attribute 'register' (most likely due to a circular import)\n",
      "\u001b[0m\n",
      "  logger.warn(f\"plugin: {plugin.value} raised {traceback.format_exc()}\")\n"
     ]
    }
   ],
   "source": [
    "from env import Environment\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3368bb9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T11:48:38.629910Z",
     "iopub.status.busy": "2025-05-16T11:48:38.629598Z",
     "iopub.status.idle": "2025-05-16T11:48:38.635479Z",
     "shell.execute_reply": "2025-05-16T11:48:38.634620Z"
    },
    "papermill": {
     "duration": 0.012003,
     "end_time": "2025-05-16T11:48:38.636953",
     "exception": false,
     "start_time": "2025-05-16T11:48:38.624950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_state(state):\n",
    "    ret_state = {}\n",
    "    # state[\"time_step\"] = np.array([state[\"time_step\"]]).astype(np.float32).flatten(0)\n",
    "    # state[\"map\"] = np.array(state[\"map\"]).astype(np.float32)\n",
    "    ret_state[\"robots\"] = np.array(state[\"robots\"]).astype(np.float32).flatten()\n",
    "    ret_state[\"packages\"] = np.array(state[\"packages\"]).astype(np.float32).flatten()[:100]\n",
    "    if len(ret_state[\"packages\"]) < 1000:\n",
    "        ret_state[\"packages\"] = np.concatenate((ret_state[\"packages\"], np.zeros(100-len(ret_state[\"packages\"]))))\n",
    "    return np.concatenate(list(ret_state.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "956682d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T11:48:38.645574Z",
     "iopub.status.busy": "2025-05-16T11:48:38.645268Z",
     "iopub.status.idle": "2025-05-16T11:48:38.649343Z",
     "shell.execute_reply": "2025-05-16T11:48:38.648489Z"
    },
    "papermill": {
     "duration": 0.009856,
     "end_time": "2025-05-16T11:48:38.650664",
     "exception": false,
     "start_time": "2025-05-16T11:48:38.640808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reward_shaping(r, env, state, action):\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81dbc4ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T11:48:38.660175Z",
     "iopub.status.busy": "2025-05-16T11:48:38.659838Z",
     "iopub.status.idle": "2025-05-16T11:48:38.668983Z",
     "shell.execute_reply": "2025-05-16T11:48:38.668080Z"
    },
    "papermill": {
     "duration": 0.015299,
     "end_time": "2025-05-16T11:48:38.670512",
     "exception": false,
     "start_time": "2025-05-16T11:48:38.655213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Avoid to modify the Env class,\n",
    "# If it is neccessary, you should describe those changes clearly in report and code\n",
    "class Env(gym.Env):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Env, self).__init__()\n",
    "        self.env = Environment(*args, **kwargs)\n",
    "\n",
    "        self.action_space = spaces.multi_discrete.MultiDiscrete([5, 3]*self.env.n_robots)\n",
    "\n",
    "\n",
    "        self.prev_state = self.env.reset()\n",
    "        first_state=convert_state(self.prev_state)\n",
    "        # Define observation space as a dictionary\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=100, shape=first_state.shape, dtype=np.float32)\n",
    "\n",
    "\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        self.le1, self.le2= LabelEncoder(), LabelEncoder()\n",
    "        self.le1.fit(['S', 'L', 'R', 'U', 'D'])\n",
    "        self.le2.fit(['0','1', '2'])\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        self.prev_state = self.env.reset()\n",
    "        return convert_state(self.prev_state), {}\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        return self.env.render()\n",
    "\n",
    "    def step(self, action):\n",
    "        ret = []\n",
    "        ret.append(self.le1.inverse_transform(action.reshape(-1, 2).T[0]))\n",
    "        ret.append(self.le2.inverse_transform(action.reshape(-1, 2).T[1]))\n",
    "        action = list(zip(*ret))\n",
    "\n",
    "        # You should not modify the infos object\n",
    "        s, r, done, infos = self.env.step(action)\n",
    "        new_r = reward_shaping(r, self.env, self.prev_state, action)\n",
    "        self.prev_state = s\n",
    "        return convert_state(s), new_r, \\\n",
    "            done, False, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc8a2e3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T11:48:38.679297Z",
     "iopub.status.busy": "2025-05-16T11:48:38.679015Z",
     "iopub.status.idle": "2025-05-16T11:48:45.736376Z",
     "shell.execute_reply": "2025-05-16T11:48:45.734993Z"
    },
    "papermill": {
     "duration": 7.063171,
     "end_time": "2025-05-16T11:48:45.737625",
     "exception": true,
     "start_time": "2025-05-16T11:48:38.674454",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/2028143641.py:59: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  state = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0).to(device) # add #sequence and #batch: [[#batch, #sequence, n_agents, n_feature]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 115 at dim 1 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/2784205691.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mhidden_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13/3218894466.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state, last_action, hidden_in, deterministic)\u001b[0m\n\u001b[1;32m     38\u001b[0m         '''\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13/2028143641.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state, last_action, hidden_in, deterministic)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mlast_action\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_agents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_shape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         '''\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add #sequence and #batch: [[#batch, #sequence, n_agents, n_feature]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         last_action = torch.LongTensor(\n\u001b[1;32m     61\u001b[0m             last_action).unsqueeze(0).unsqueeze(0).to(device)  # add #sequence and #batch: [#batch, #sequence, n_agents, action_shape]\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 115 at dim 1 (got 0)"
     ]
    }
   ],
   "source": [
    "replay_buffer_size = 1e4\n",
    "hidden_dim = 64\n",
    "hypernet_dim = 128\n",
    "max_steps = 100\n",
    "max_episodes = 1000\n",
    "update_iter  = 1\n",
    "batch_size = 2\n",
    "save_interval = 10\n",
    "target_update_interval = 10\n",
    "model_path = 'model/qmix'\n",
    "\n",
    "env = Env('map2.txt', 100, 5, 20, -0.01, 10., 1., 10)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_agents = 5\n",
    "action_dim = 5*3*n_agents\n",
    "action_shape = 1\n",
    "\n",
    "replay_buffer = ReplayBufferGRU(replay_buffer_size)\n",
    "learner = QMix_Trainer(replay_buffer, n_agents, state_dim, action_shape, action_dim, hidden_dim, hypernet_dim, target_update_interval)\n",
    "\n",
    "loss = None\n",
    "\n",
    "for epi in range(max_episodes):\n",
    "    hidden_out = torch.zeros([1, n_agents, hidden_dim], dtype=torch.float).to(device)\n",
    "    last_action = learner.sample_action()\n",
    "    episode_state = []\n",
    "    episode_action = []\n",
    "    episode_last_action = []\n",
    "    episode_reward = []\n",
    "    episode_next_state = []\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        hidden_in = hidden_out\n",
    "        action, hidden_out = learner.get_action(state, last_action, hidden_in)\n",
    "\n",
    "        action = action.reshape(-1)\n",
    "        ret = []\n",
    "        ret.append(le1.inverse_transform(action.reshape(-1, 2).T[0]))\n",
    "        ret.append(le2.inverse_transform(action.reshape(-1, 2).T[1]))\n",
    "        action = list(zip(*ret))\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        if step == 0:\n",
    "            ini_hidden_in = hidden_in\n",
    "            ini_hidden_out = hidden_out\n",
    "\n",
    "        episode_state.append(state)\n",
    "        episode_action.append(action)\n",
    "        episode_last_action.append(last_action)\n",
    "        episode_reward.append(reward)\n",
    "        episode_next_state.append(next_state)\n",
    "\n",
    "        state = next_state\n",
    "        last_action = action\n",
    "\n",
    "        if np.any(done):\n",
    "            break\n",
    "\n",
    "    if args.train:\n",
    "        learner.push_replay_buffer(ini_hidden_in, ini_hidden_out, episode_state, episode_action, episode_last_action,\n",
    "                            episode_reward, episode_next_state)\n",
    "        if epi > batch_size:\n",
    "            for _ in range(update_iter):\n",
    "                loss = learner.update(batch_size)\n",
    "\n",
    "        if epi % save_interval == 0:\n",
    "            learner.save_model(model_path)\n",
    "\n",
    "    print(f\"Episode: {epi}, Episode Reward: {np.sum(episode_reward)}, Loss: {loss}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26.116338,
   "end_time": "2025-05-16T11:48:48.617806",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-16T11:48:22.501468",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
